{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Chatbot\n",
    "\n",
    "This *Jupyter Notebook* is dedicated to the study of the **DeepSeek Model** to create an interactive chatbot. \n",
    "\n",
    "The following study will be developed by the implementation of *Python* code using *DeepSeek* and *ollama* API. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing modules...\n",
    "import re\n",
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "\n",
    "import sys\n",
    "sys.path.append('..') # Go back to base directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large Language Models\n",
    "\n",
    "A **Large Language Model (LLM)** is a type of artificial intelligence (AI) model designed to understand, generate, and manipulate human-like text. These models are trained on vast amounts of text data and use deep learning techniques, particularly transformer architectures, to predict and generate coherent language.\n",
    "\n",
    "The definition above for instance was generated by DeepSeek!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepSeek's Local Installation\n",
    "\n",
    "To use DeepSeek locally in Python or other applications, you'll need to install it. For this:\n",
    "\n",
    "1. **Download ollama: https://ollama.com/download ;**\n",
    "2. **Install ollama;**\n",
    "3. **On your CMD:** `ollama run deepseek-r1`\n",
    "4. **Try using the AI on your CMD;**\n",
    "5. **On your Python Environment:** `pip install ollama`.\n",
    "\n",
    "You're now ready to use it!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSeekLLM:\n",
    "    def __init__(\n",
    "        self, \n",
    "        verbose=False\n",
    "    ):\n",
    "        self.verbose = verbose # Toggle for debug messages\n",
    "\n",
    "\n",
    "    def chat_completion(\n",
    "        self, \n",
    "        system_role,\n",
    "        prompt, \n",
    "        deep_think=False\n",
    "    ):\n",
    "        # Generate messages\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\", \n",
    "                \"content\": system_role\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        response: ChatResponse = chat(\n",
    "            model='deepseek-r1', \n",
    "            messages=messages\n",
    "        )\n",
    "        \n",
    "        # Parse response\n",
    "        response_text = response['message']['content']\n",
    "\n",
    "        # Extract everything inside <think>...</think> - this is the Deep Think\n",
    "        think_texts = re.findall(r'<think>(.*?)</think>', response_text, flags=re.DOTALL)\n",
    "\n",
    "        # Join extracted sections (optional, if multiple <think> sections exist)\n",
    "        think_texts = \"\\n\\n\".join(think_texts).strip()\n",
    "        \n",
    "        # Exclude the Deep Think, and return the response\n",
    "        clean_response= re.sub(r'<think>.*?</think>', '', response_text, flags=re.DOTALL).strip()\n",
    "\n",
    "        # Return either the context, or a tuple with the context and deep think\n",
    "        return clean_response if not deep_think else (clean_response, think_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: 'Hello!'\n",
      "LLM: 'Hello! How can I assist you today? ðŸ˜Š'\n",
      "User: 'I need to go. Bye!'\n",
      "LLM: 'Bye Bye!'\n"
     ]
    }
   ],
   "source": [
    "LLM = DeepSeekLLM(\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "system_role = \"\"\"\n",
    "You are a voice-based AI chatbot that engages in friendly and natural conversations.  \n",
    "You listen to user speech inputs and respond clearly and concisely.  \n",
    "If the user hints they don't want to continue (e.g., \"I'm done,\" \"Goodbye,\" \"I don't want to talk anymore\"), respond only with: \"Bye Bye!\"  \n",
    "Otherwise, keep the conversation flowing naturally.  \n",
    "Your tone should be polite, engaging, and easy to understand in spoken form.\n",
    "\"\"\"\n",
    "\n",
    "user_input, response = \"\", \"\"\n",
    "\n",
    "while response != \"Bye Bye!\":\n",
    "    user_input = input(\"User: \")\n",
    "\n",
    "    if user_input:\n",
    "        print(f\"User: '{user_input}'\")\n",
    "\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    response = LLM.chat_completion(\n",
    "        system_role=system_role,\n",
    "        prompt=user_input\n",
    "    )\n",
    "\n",
    "    print(f\"LLM: '{response}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
